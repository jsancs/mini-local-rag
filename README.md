# mini-local-rag

A tiny implementation of a RAG system that runs entirely on your computer!

### Usage

1. Start Ollama engine.
2. Run `python cli.py`. It will run a Llama3.2:1b model.
3. Chat with the model

There is 1 configurable param: <br>
* `-m --model`: model to use (llama3.2:1b by default). You can check the full list of available models [here](https://ollama.com/library)